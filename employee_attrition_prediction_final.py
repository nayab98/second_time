# -*- coding: utf-8 -*-
"""employee_attrition_prediction_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vgqxb7nMSm5aln73M_fBhqIKYlh-nLE7
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

df=pd.read_csv('Employee_Attrition (6).csv')

df

df.dtypes

df.describe()

df.isnull().sum()

plt.figure(figsize=(12,12))
sns.heatmap(df.corr(),annot=True, fmt='.0%')

df=df.drop(['EmployeeCount','StandardHours'], axis=1)

df

df=df.drop(['EmployeeNumber','Over18'], axis=1)

df.dtypes

for column in df.columns:
  if df[column].dtype == object:
    print(str(column) + ':' + str(df[column].unique()))
    print(df[column].value_counts())
    print('_____________________')

from sklearn import preprocessing
lab_enc=preprocessing.LabelEncoder()

categorical_column = ['Attrition', 'Gender', 'OverTime']

data_encoded = df.copy(deep=True)
for col in categorical_column:
    data_encoded[col] = lab_enc.fit_transform(df[col])
    le_name_mapping = dict(zip(lab_enc.classes_, lab_enc.transform(lab_enc.classes_)))
    print('Feature', col)
    print('mapping', le_name_mapping)
    print('_______________________')

data_encoded.dtypes

data_encoded

df_mod=data_encoded

df_mod.dtypes

for column in df_mod.columns:
  if df_mod[column].dtype == object:
    print(str(column) + ':' + str(df_mod[column].unique()))
    print(df_mod[column].value_counts())
    print('_____________________')

df_mod=pd.get_dummies(df_mod, columns=["MaritalStatus","JobRole","EducationField","Department","BusinessTravel"])

df_mod

df1=df_mod

x=df_mod.drop('Attrition',1)

x

y=df1.iloc[:,1].values

y

from sklearn.ensemble import ExtraTreesClassifier
model = ExtraTreesClassifier()
model.fit(x,y)

print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers
#plot graph of feature importances for better visualization
feat_importances = pd.Series(model.feature_importances_, index=x.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.show()

x.columns

x=x.iloc[:,[0,2,4,9,10,13,16,18,20,24,]]
x

#from sklearn.preprocessing import StandardScaler
#std=StandardScaler()
#x=std.fit_transform(x)
#x=std.fit_transform(x)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.25, random_state=0)

from sklearn.ensemble import RandomForestClassifier
forest=RandomForestClassifier(n_estimators=19, criterion='entropy', random_state=0)
forest.fit(x_train, y_train)
forest.score(x_test, y_test)

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
n_estimators = list(range(1,50))
#Convert to dictionary
hyperparameters = dict(n_estimators=n_estimators)
classifier1= RandomForestClassifier()
#Use GridSearch
clf = GridSearchCV(classifier1, hyperparameters, cv=10)
#Fit the model
best_model = clf.fit(x_train,y_train)
#Print The value of best Hyperparameters
print('Best n_estimators:', best_model.best_estimator_.get_params()['n_estimators'])

pred=forest.predict(x_test)
from sklearn.metrics import confusion_matrix,classification_report
print(classification_report(y_test, pred))

from sklearn.metrics import confusion_matrix

data = confusion_matrix(y_test,pred)
df_cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test))
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (6,6))
sns.set(font_scale=1.4)#for label size
sns.heatmap(df_cm, annot=True,annot_kws={"size": 12})# font size

from sklearn.linear_model import LogisticRegression 
model = LogisticRegression(random_state=0)
model.fit(x_train,y_train)
pred1=model.predict(x_test)
pred2=model.score(x_test,y_test)
pred2

print(classification_report(y_test, pred1))

from sklearn.metrics import confusion_matrix

data = confusion_matrix(y_test, pred1)
df_cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test))
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (6,6))
sns.set(font_scale=1.4)#for label size
sns.heatmap(df_cm, annot=True,annot_kws={"size": 12})# font size

from sklearn.tree import DecisionTreeClassifier
classifier_dec = DecisionTreeClassifier(criterion = 'entropy',random_state=0)
classifier_dec.fit(x_train,y_train)
y_dec= classifier_dec.predict(x_test)
y_dec1=classifier_dec.score(x_test,y_test)
y_dec1

print(classification_report(y_test, y_dec))

from sklearn.metrics import confusion_matrix

data = confusion_matrix(y_test, y_dec)
df_cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test))
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (6,6))
sns.set(font_scale=1.4)#for label size
sns.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 12})# font size

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=2)
neigh.fit(x_train,y_train)
kpred=neigh.predict(x_test)
kpred1=neigh.score(x_test,y_test)
kpred1

print(classification_report(y_test,kpred))

from sklearn.metrics import confusion_matrix

data = confusion_matrix(y_test, kpred)
df_cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test))
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (6,6))
sns.set(font_scale=1.4)#for label size
sns.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 12})# font size

#error for knn
error_rate = []


for i in range(1,30):
    
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(x_train,y_train)
    pred_i = knn.predict(x_test)
    error_rate.append(np.mean(pred_i != y_test))

plt.figure(figsize=(10,6))
plt.plot(range(1,30),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

knn = KNeighborsClassifier(n_neighbors=9)

knn.fit(x_train,y_train)
kpred=knn.predict(x_test)
kpred1=knn.score(x_test,y_test)
kpred1

import pickle

pickle.dump(forest, open('model.pkl','wb'))

model = pickle.load(open('model.pkl','rb'))

model.predict([[41,	1	,2	,4	,5993	,1	,1	,8	,1,	5]])



